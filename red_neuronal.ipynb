{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RED NEURONAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta es una red neuronal simple construida desde cero usando solo NumPy. Se compone de una capa oculta con 128 neuronas y ReLU como función de activación, y una capa de salida con Softmax para clasificación en 10 categorías del dataset Fashion MNIST."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Importamos las librerias necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1 Carga\n",
    "\n",
    " Se carga el dataset Fashion MNIST "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el dataset Fashion MNIST\n",
    "(train_X, train_y), (test_X, test_y) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- train_X, train_y: imágenes y etiquetas para el entrenamiento .\n",
    "- test_X, test_y: imágenes y etiquetas para el test ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 2 Normalizacion de datos\n",
    "\n",
    "Se normalizan los valores de píxeles a un rango de 0 a 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizar los datos (0-1) y convertir a 1D\n",
    "X_train = train_X.reshape(-1, 28 * 28) / 255.0\n",
    "X_test = test_X.reshape(-1, 28 * 28) / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- train_X.reshape(-1, 28 * 28): Convierte cada imagen de 28x28 en un vector de 784 elementos .\n",
    "- / 255.0: Normaliza los valores de píxeles (de 0-255 a 0-1), lo que ayuda al entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 3 One-Hot Encoding\n",
    "\n",
    "Las etiquetas se convierten a formato one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir etiquetas a codificación one-hot\n",
    "def one_hot(y, num_classes=10):\n",
    "    return np.eye(num_classes)[y]\n",
    "\n",
    "y_train_one_hot = one_hot(train_y)\n",
    "y_test_one_hot = one_hot(test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Las redes neuronales trabajan mejor con etiquetas categóricas en formato One-Hot.\n",
    "- np.eye(num_classes)[y]crea un array de 10 elementos donde solo el índice de la clase es 1, el resto 0.\n",
    "- Ejemplo:\n",
    "Si train_y[0] = 3, su versión one-hot es:\n",
    "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 4 Division en entrenamiento y validacion\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir en entrenamiento y validación\n",
    "X_val, y_val = X_train[50000:], y_train_one_hot[50000:]\n",
    "X_train, y_train = X_train[:50000], y_train_one_hot[:50000]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Toma los últimos 10,000 ejemplos como conjunto de validación.\n",
    "- Usa los primeros 50,000 ejemplos para entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 5 Implementacion de la Red Neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, n_inputs=784, n_outputs=10, hidden_units=128, reg=0.001):\n",
    "        np.random.seed(0)\n",
    "        self.reg = reg  # Regularización L2\n",
    "\n",
    "        # Pesos y sesgos de la capa oculta\n",
    "        self.w1 = np.random.randn(hidden_units, n_inputs) / np.sqrt(n_inputs)\n",
    "        self.b1 = np.zeros(hidden_units)\n",
    "\n",
    "        # Pesos y sesgos de la capa de salida\n",
    "        self.w2 = np.random.randn(n_outputs, hidden_units) / np.sqrt(hidden_units)\n",
    "        self.b2 = np.zeros(n_outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Creamos una clase NeuralNetworkcon:\n",
    "  - 784 entradas (28x28 píxeles).\n",
    "  - 128 neuronas ocultas .\n",
    "  - 10 neuronas de salida (una por cada número del 0 al 9).\n",
    "- Se inicializan los pesos ( w1, w2) y los sesgos ( b1, b2).\n",
    "- np.random.randn(...) / np.sqrt(...)es una técnica de inicialización de pesos llamada Xavier Inicialización que ayuda a mejorar el aprendizaje.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Funciones de activacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(self, x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def softmax(self, x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exp_x / exp_x.sum(axis=1, keepdims=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1. ReLU (Unidad lineal rectificada)\n",
    "  - Se usa en la capa oculta .\n",
    "  - max(0, x): Si el valor es negativo, se convierte en 0.\n",
    "- 2. Máximo suave\n",
    "  -  Se usa en la capa de salida .\n",
    "  - Convierte los valores en probabilidades (la suma es 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 6 Propagacion hacia adelante (Paso hacia adelante)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, X):\n",
    "    self.h = self.relu(X @ self.w1.T + self.b1)\n",
    "    self.output = self.softmax(self.h @ self.w2.T + self.b2)\n",
    "    return np.argmax(self.output, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. X @ self.w1.T + self.b1: Multiplicamos los datos por los pesos y sumamos los sesgos.\n",
    "2. self.relu(...):Aplicamos la función ReLU.\n",
    "3. self.h @ self.w2.T + self.b2: Pasamos la salida de la capa oculta a la capa de salida.\n",
    "4. self.softmax(...): Convertimos en probabilidades."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 7  Propagacion hacia atras (Paso hacia atras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(self, X, Y, X_val, Y_val, epochs=50, lr=0.01):\n",
    "    train_losses, val_losses = [], []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # FORWARD PASS\n",
    "        h = self.relu(X @ self.w1.T + self.b1)\n",
    "        f = self.softmax(h @ self.w2.T + self.b2)\n",
    "\n",
    "        # BACKPROPAGATION\n",
    "        d_out = f - Y\n",
    "        d_h = (h > 0) * (d_out @ self.w2)\n",
    "\n",
    "        Dw2 = d_out.T @ h / len(X) + self.reg * self.w2\n",
    "        Db2 = d_out.mean(axis=0)\n",
    "        Dw1 = d_h.T @ X / len(X) + self.reg * self.w1\n",
    "        Db1 = d_h.mean(axis=0)\n",
    "\n",
    "        # ACTUALIZAR PESOS\n",
    "        self.w1 -= lr * Dw1\n",
    "        self.b1 -= lr * Db1\n",
    "        self.w2 -= lr * Dw2\n",
    "        self.b2 -= lr * Db2\n",
    "\n",
    "        # CALCULAR PÉRDIDAS\n",
    "        loss = -np.mean(np.sum(Y * np.log(f + 1e-8), axis=1))\n",
    "        val_pred = self.softmax(self.relu(X_val @ self.w1.T + self.b1) @ self.w2.T + self.b2)\n",
    "        val_loss = -np.mean(np.sum(Y_val * np.log(val_pred + 1e-8), axis=1))\n",
    "\n",
    "        train_losses.append(loss)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        print(f\"Época {epoch + 1}, Pérdida: {loss:.4f}, Validación: {val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Se calculan los gradientes de los pesos ( Dw1, Dw2) y sesgos ( Db1, Db2).\n",
    "2. Se actualizan los pesos con lr(tasa de aprendizaje).\n",
    "3. Se mide la función de pérdida en entrenamiento y validación.\n",
    "4. Se imprimen los resultados por época."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hemos construido una red neuronal desde cero utilizando únicamente NumPy para clasificar imágenes del dataset Fashion MNIST. A continuación, se detalla cómo hemos cumplido con los requisitos obligatorios y las habilidades necesarias:\n",
    "\n",
    "- Arquitectura de la Red Neuronal:\n",
    "\n",
    "Diseñamos una red neuronal con una capa oculta de 128 neuronas y una capa de salida de 10 neuronas, adecuada para la clasificación de las 10 categorías del dataset Fashion MNIST.\n",
    "- Propagación hacia Adelante y Hacia Atrás:\n",
    "\n",
    "Implementamos la propagación hacia adelante (forward propagation) para calcular las salidas de la red.\n",
    "Implementamos la retropropagación (backpropagation) para calcular los gradientes y ajustar los pesos y sesgos de la red.\n",
    "- Funciones de Activación:\n",
    "\n",
    "Utilizamos la función de activación ReLU en la capa oculta para introducir no linealidad.\n",
    "Utilizamos la función de activación Softmax en la capa de salida para obtener probabilidades de clasificación.\n",
    "- Función de Pérdida:\n",
    "\n",
    "Implementamos la función de pérdida de entropía cruzada para medir el error entre las predicciones y las etiquetas verdaderas.\n",
    "- Descenso de Gradiente:\n",
    "\n",
    "Utilizamos el descenso de gradiente para actualizar los pesos y sesgos de la red en cada iteración del entrenamiento.\n",
    "- Entrenamiento y Evaluación:\n",
    "\n",
    "Entrenamos la red neuronal durante 50 épocas, observando una disminución constante en la pérdida.\n",
    "Evaluamos el modelo en el conjunto de prueba, obteniendo una precisión adecuada para una red neuronal simple."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
